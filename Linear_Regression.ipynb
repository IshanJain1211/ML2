{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "-zTLHrFCT6KY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aim is to predict the marks of students of the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "p0KHq8ZgTpU4",
    "outputId": "af767fa4-341f-4e19-abcd-e9fbbecd9c86"
   },
   "outputs": [],
   "source": [
    "# Use the file namd 'training data' to train the model\n",
    "\n",
    "data = pd.read_excel('Training data.xlsx')\n",
    "\n",
    "x_train = np.array(data.iloc[:,0:8])\n",
    "y_train = np.array(data.iloc[:,8]).reshape(-1,1)\n",
    "\n",
    "# Try plotting y_train with different features\n",
    "# To get an idea whether to add some features or not\n",
    "# Add some features if required in x_train\n",
    "\n",
    "# Also do label encoding for features not represented in numbers\n",
    "# refer the link if not know : https://youtu.be/589nCGeWG1w?si=t2Wa7LgbUOO4RooM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['no', 'M', 2, 2, 5, 6, 18, 118],\n",
       "       ['yes', 'M', 2, 2, 2, 7, 19, 107]], dtype=object)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_changing(x_train):\n",
    "  # ---------\n",
    "       m=x_train.shape[0]\n",
    "       for i in range(m):\n",
    "           if x_train[i][1]=='M':\n",
    "               x_train[i][1]=1\n",
    "           if x_train[i][0]=='yes':\n",
    "               x_train[i][0]=1\n",
    "           \n",
    "           if x_train[i][1]=='F':\n",
    "                x_train[i][1]=0\n",
    "           if x_train[i][0]=='no':\n",
    "               x_train[i][0]=0\n",
    "           \n",
    "               \n",
    "      # --------\n",
    "       return x_train\n",
    "\n",
    "x_train = feature_changing(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "tYshvtYlVour"
   },
   "outputs": [],
   "source": [
    "def z_score(x_train):\n",
    "\n",
    "  # ---------\n",
    "    # write the code for feature scaling here\n",
    "    # Your code here\n",
    "    x_mean=np.mean(x_train,axis=0)\n",
    "    x_std=np.std(x_train,axis=0)\n",
    "    x_train=(x_train-x_mean)/x_std\n",
    "  # ---------\n",
    "\n",
    "    return x_train,x_std,x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "O5dOwbNbWJWa"
   },
   "outputs": [],
   "source": [
    "def cost(x_train,y_train,w1,b1,w2,b2):\n",
    "   \n",
    "  # ---------\n",
    "    # Your code here\n",
    "    m=x_train.shape[0]\n",
    "    value1=x_train@w1+b1 #(1000,100)\n",
    "    value2=value1@w2+b2   #(1000,1)\n",
    "    a=(value2-y_train)**2\n",
    "    summ=a.sum(axis=0)\n",
    "    loss=summ/(2*m)\n",
    "    \n",
    "    # Use mean square error as cost function\n",
    "    # return cost\n",
    "  # ---------\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "hW8p2cTNU74W"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x_train,y_train,w1,b1,w2,b2):\n",
    "\n",
    "  # ---------\n",
    "    # Your code here\n",
    "    \n",
    "    c=0.01\n",
    "               \n",
    "    m=x_train.shape[0]\n",
    "    value1=x_train@w1+b1 #(1000,100)\n",
    "    value2=value1@w2+b2   #(1000,1)\n",
    "    summ=(value2-y_train)  #1000,1\n",
    "\n",
    "\n",
    "    \n",
    "    w2_d=(np.transpose(value1))@(summ)  #(100,1)\n",
    "    w2_d=w2_d/m\n",
    "    b2_d=summ.sum(axis=0)/m\n",
    "    dvalue1=(summ@(np.transpose(w2)))/m   #(1000,100)\n",
    "    w1_d=((np.transpose(x_train))@dvalue1)\n",
    "    b1_d=dvalue1.sum(axis=0)\n",
    "    w2=w2-c*w2_d\n",
    "    b2=b2-c*b2_d\n",
    "    w1=w1-c*w1_d\n",
    "    b1=b1-c*b1_d\n",
    "     # Check for NaNs after updating\n",
    "    if np.isnan(w1).any() or np.isnan(b1).any() or np.isnan(w2).any() or np.isnan(b2).any():\n",
    "        print(\"NaN encountered in weights or biases.\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # Choose learning rate yourself\n",
    "  # ---------\n",
    "\n",
    "    return w1,b1,w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float64)\n",
    "x_train,x_std,x_mean = z_score(x_train)\n",
    "\n",
    "np.random.seed(2147483647)\n",
    "w1 = np.random.randn(x_train.shape[1],100)\n",
    "w2=np.random.randn(100,1)\n",
    "b1 = np.random.randn(1,100)\n",
    "b2=np.random.randn(1)\n",
    "old_cost = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "Kl-fioJ5WkYn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.4977258e-06]\n",
      "Optimization required, your accuracy is 72.5%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "  \n",
    "  w1,b1,w2,b2 = gradient_descent(x_train,y_train,w1,b1,w2,b2)\n",
    "  if np.isnan(current_cost):\n",
    "        print(\"NaN encountered in cost.\")\n",
    "        break\n",
    "  \n",
    "\n",
    "c=cost(x_train,y_train,w1,b1,w2,b2)\n",
    "print(c)\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_predict = pd.read_excel('Test data.xlsx').iloc[:,:8].to_numpy()\n",
    "\n",
    "x_predict = feature_changing(x_predict)\n",
    "x_predict = x_predict.astype(np.float64)\n",
    "x_predict = (x_predict - x_mean)/x_std\n",
    "ans = pd.read_excel('Test data.xlsx').iloc[:,8].to_numpy()\n",
    "\n",
    "\n",
    "value1= np.dot(x_predict,w1) + b1\n",
    "y_predict=np.dot(value1,w2)+b2\n",
    "\n",
    "\n",
    "accuracy = 0\n",
    "for dim in range(len(ans)):\n",
    "  if abs(y_predict[dim]-ans[dim])<0.5: # do not change the tolerance as you'll be checked on +- 0.5 error only\n",
    "    accuracy += 1\n",
    "accuracy = round(accuracy*100/200.0,2)\n",
    "ok = 'Congratulations' if accuracy>95 else 'Optimization required'\n",
    "print(f\"{ok}, your accuracy is {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Cost: 5.497725805000927e-06\n",
      "[74.59640421] : 74.59\n",
      "[65.28516836] : 65.28\n",
      "[79.23663302] : 79.24\n",
      "[62.65770937] : 62.66\n",
      "[67.82597738] : 67.83\n",
      "[75.42654302] : 75.43\n",
      "[77.05747124] : 77.06\n",
      "[75.94546637] : 75.94\n",
      "[72.51067953] : 72.51\n",
      "[71.60558554] : 71.61\n",
      "[65.03807035] : 65.03\n",
      "[59.08599087] : 59.09\n",
      "[70.00643936] : 70.0\n",
      "[59.0624594] : 59.06\n",
      "[70.12080667] : 70.12\n",
      "[74.38731654] : 74.39\n",
      "[57.70728375] : 57.71\n",
      "[79.33696807] : 79.34\n",
      "[57.53270518] : 57.53\n",
      "[60.87396966] : 60.87\n",
      "[74.70237919] : 74.7\n",
      "[63.70575002] : 63.7\n",
      "[79.42120072] : 79.42\n",
      "[64.51612193] : 64.52\n",
      "[79.5159122] : 79.52\n",
      "[85.40601279] : 85.41\n",
      "[68.13677837] : 68.14\n",
      "[68.27723572] : 68.28\n",
      "[62.30776569] : 62.31\n",
      "[82.25537006] : 82.25\n",
      "[61.25162447] : 61.25\n",
      "[69.06280178] : 69.06\n",
      "[78.41074699] : 78.41\n",
      "[75.79569351] : 75.8\n",
      "[80.10719038] : 80.11\n",
      "[69.74018794] : 69.74\n",
      "[77.03521502] : 77.03\n",
      "[75.39994914] : 75.4\n",
      "[78.28640207] : 78.28\n",
      "[68.64124969] : 68.64\n",
      "[69.31678542] : 69.31\n",
      "[61.76610981] : 61.77\n",
      "[77.70250001] : 77.7\n",
      "[74.52525095] : 74.53\n",
      "[67.63781913] : 67.64\n",
      "[64.55760442] : 64.56\n",
      "[66.14138476] : 66.14\n",
      "[71.91453458] : 71.91\n",
      "[75.14756216] : 75.15\n",
      "[74.09986056] : 74.1\n",
      "[62.40068821] : 62.4\n",
      "[64.42536061] : 64.43\n",
      "[75.73065294] : 75.73\n",
      "[71.52252956] : 71.52\n",
      "[80.92072443] : 80.92\n",
      "[61.66141105] : 61.66\n",
      "[71.92579808] : 71.93\n",
      "[85.23707286] : 85.24\n",
      "[70.30538682] : 70.31\n",
      "[79.30592465] : 79.31\n",
      "[69.64596708] : 69.65\n",
      "[70.62231203] : 70.62\n",
      "[68.92420639] : 68.92\n",
      "[75.74064286] : 75.74\n",
      "[62.696684] : 62.7\n",
      "[74.37546393] : 74.38\n",
      "[64.17751364] : 64.18\n",
      "[65.24492119] : 65.25\n",
      "[63.89208179] : 63.89\n",
      "[74.55134362] : 74.55\n",
      "[68.2611331] : 68.26\n",
      "[76.64589583] : 76.65\n",
      "[62.56172444] : 62.56\n",
      "[58.35796477] : 58.36\n",
      "[67.90586049] : 67.91\n",
      "[65.81315666] : 65.81\n",
      "[63.33243644] : 63.33\n",
      "[64.29703004] : 64.3\n",
      "[55.67788317] : 55.68\n",
      "[72.48227068] : 72.48\n",
      "[74.13156712] : 74.13\n",
      "[65.27197803] : 65.27\n",
      "[58.40128896] : 58.4\n",
      "[76.01605898] : 76.02\n",
      "[71.63217943] : 71.63\n",
      "[68.84373557] : 68.84\n",
      "[61.17174137] : 61.17\n",
      "[68.71759202] : 68.72\n",
      "[60.03693291] : 60.04\n",
      "[60.18362966] : 60.18\n",
      "[79.69050322] : 79.69\n",
      "[72.60052594] : 72.6\n",
      "[74.78582624] : 74.78\n",
      "[75.17285415] : 75.17\n",
      "[75.78087771] : 75.78\n",
      "[80.77143863] : 80.77\n",
      "[83.90474194] : 83.91\n",
      "[78.67015806] : 78.67\n",
      "[67.93738058] : 67.94\n",
      "[59.27362191] : 59.27\n",
      "[68.67150694] : 68.67\n",
      "[74.24594595] : 74.25\n",
      "[77.94571062] : 77.95\n",
      "[78.71156774] : 78.71\n",
      "[70.17536855] : 70.18\n",
      "[79.4961453] : 79.5\n",
      "[73.89192203] : 73.89\n",
      "[75.22054377] : 75.22\n",
      "[71.58888013] : 71.59\n",
      "[71.14916646] : 71.15\n",
      "[62.72022792] : 62.72\n",
      "[69.80139182] : 69.8\n",
      "[80.18015874] : 80.18\n",
      "[80.07673343] : 80.08\n",
      "[69.25667186] : 69.25\n",
      "[72.17204543] : 72.17\n",
      "[60.43632819] : 60.44\n",
      "[80.49522219] : 80.5\n",
      "[60.3612985] : 60.36\n",
      "[70.4973576] : 70.5\n",
      "[72.98241641] : 72.98\n",
      "[73.29805573] : 73.3\n",
      "[71.36112795] : 71.36\n",
      "[80.23650945] : 80.24\n",
      "[70.28494561] : 70.28\n",
      "[71.50897477] : 71.51\n",
      "[71.36104006] : 71.36\n",
      "[59.74104805] : 59.74\n",
      "[54.6531032] : 54.65\n",
      "[69.19548098] : 69.19\n",
      "[72.09780453] : 72.09\n",
      "[76.13923875] : 76.14\n",
      "[77.17571327] : 77.18\n",
      "[65.59714167] : 65.59\n",
      "[73.94214601] : 73.94\n",
      "[62.48542208] : 62.48\n",
      "[76.79073082] : 76.79\n",
      "[62.22915868] : 62.23\n",
      "[61.20141215] : 61.2\n",
      "[79.38405614] : 79.38\n",
      "[66.53195527] : 66.53\n",
      "[68.83633526] : 68.84\n",
      "[64.94771106] : 64.95\n",
      "[65.39280707] : 65.39\n",
      "[56.51363403] : 56.51\n",
      "[76.37521955] : 76.38\n",
      "[68.58984996] : 68.59\n",
      "[65.2597628] : 65.26\n",
      "[67.47301797] : 67.47\n",
      "[68.10710667] : 68.11\n",
      "[85.33661022] : 85.34\n",
      "[70.07750737] : 70.08\n",
      "[71.26511549] : 71.27\n",
      "[75.28065654] : 75.28\n",
      "[79.86065808] : 79.86\n",
      "[74.98526045] : 74.98\n",
      "[62.42560592] : 62.43\n",
      "[68.07606234] : 68.08\n",
      "[75.46136197] : 75.46\n",
      "[63.07107485] : 63.07\n",
      "[74.36733657] : 74.37\n",
      "[68.61773066] : 68.62\n",
      "[68.13852907] : 68.14\n",
      "[74.12108922] : 74.12\n",
      "[62.97150028] : 62.97\n",
      "[66.51585081] : 66.52\n",
      "[71.36159011] : 71.36\n",
      "[79.44474464] : 79.44\n",
      "[76.13616264] : 76.14\n",
      "[70.73395351] : 70.73\n",
      "[78.09133306] : 78.09\n",
      "[59.72493299] : 59.73\n",
      "[73.25605865] : 73.25\n",
      "[55.6778592] : 55.68\n",
      "[70.67067424] : 70.67\n",
      "[79.81440915] : 79.81\n",
      "[61.30738713] : 61.31\n",
      "[77.59603718] : 77.59\n",
      "[75.21567791] : 75.22\n",
      "[76.80042426] : 76.8\n",
      "[59.29154041] : 59.29\n",
      "[61.00201436] : 61.0\n",
      "[68.38550198] : 68.39\n",
      "[71.72323907] : 71.72\n",
      "[59.6970378] : 59.7\n",
      "[59.61104371] : 59.61\n",
      "[78.55569305] : 78.56\n",
      "[66.13878511] : 66.14\n",
      "[62.87989152] : 62.88\n",
      "[57.25775122] : 57.26\n",
      "[71.29172274] : 71.29\n",
      "[73.44192563] : 73.44\n",
      "[75.13579481] : 75.14\n",
      "[73.20516014] : 73.21\n",
      "[72.60052423] : 72.6\n",
      "[77.47156967] : 77.47\n",
      "[64.98609659] : 64.98\n",
      "[72.89650758] : 72.9\n",
      "[74.59644155] : 74.59\n",
      "[74.83239746] : 74.83\n",
      "Congratulations, your accuracy is 100.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to change categorical features to numerical\n",
    "def feature_changing(x_train):\n",
    "    m = x_train.shape[0]\n",
    "    for i in range(m):\n",
    "        x_train[i][1] = 1 if x_train[i][1] == 'M' else 0\n",
    "        x_train[i][0] = 1 if x_train[i][0] == 'yes' else 0\n",
    "    return x_train\n",
    "\n",
    "# Function to normalize the data\n",
    "def z_score(x_train):\n",
    "    x_mean = np.mean(x_train, axis=0)\n",
    "    x_std = np.std(x_train, axis=0)\n",
    "    x_train = (x_train - x_mean) / x_std\n",
    "    return x_train, x_std, x_mean\n",
    "\n",
    "# Cost function\n",
    "def cost(x_train, y_train, w1, b1, w2, b2):\n",
    "    m = x_train.shape[0]\n",
    "    value1 = x_train @ w1 + b1  # (1000, 100)\n",
    "    value2 = value1 @ w2 + b2   # (1000, 1)\n",
    "    loss = np.sum((value2 - y_train) ** 2) / (2 * m)\n",
    "    return loss\n",
    "\n",
    "# Gradient Descent function\n",
    "def gradient_descent(x_train, y_train, w1, b1, w2, b2, learning_rate):\n",
    "    m = x_train.shape[0]\n",
    "    value1 = x_train @ w1 + b1  # (1000, 100)\n",
    "    value2 = value1 @ w2 + b2   # (1000, 1)\n",
    "    summ = value2 - y_train     # (1000, 1)\n",
    "\n",
    "    w2_d = (value1.T @ summ) / m  # (100, 1)\n",
    "    b2_d = np.sum(summ, axis=0) / m  # (1,)\n",
    "\n",
    "    dvalue1 = (summ @ w2.T) / m  # (1000, 100)\n",
    "    w1_d = (x_train.T @ dvalue1)  # (n_features, 100)\n",
    "    b1_d = np.sum(dvalue1, axis=0)  # (100,)\n",
    "\n",
    "    # Check for NaNs before updating\n",
    "    if np.isnan(w1_d).any() or np.isnan(b1_d).any() or np.isnan(w2_d).any() or np.isnan(b2_d).any():\n",
    "        print(\"NaN encountered in gradients.\")\n",
    "        return w1, b1, w2, b2\n",
    "\n",
    "    # Update weights and biases\n",
    "    w2 -= learning_rate * w2_d\n",
    "    b2 -= learning_rate * b2_d\n",
    "    w1 -= learning_rate * w1_d\n",
    "    b1 -= learning_rate * b1_d\n",
    "\n",
    "    # Check for NaNs after updating\n",
    "    if np.isnan(w1).any() or np.isnan(b1).any() or np.isnan(w2).any() or np.isnan(b2).any():\n",
    "        print(\"NaN encountered in weights or biases.\")\n",
    "    \n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "# Load training data\n",
    "data = pd.read_excel('Training data.xlsx')\n",
    "x_train = np.array(data.iloc[:, 0:8])\n",
    "y_train = np.array(data.iloc[:, 8]).reshape(-1, 1)\n",
    "\n",
    "# Apply feature changes\n",
    "x_train = feature_changing(x_train)\n",
    "\n",
    "# Standardize x_train\n",
    "x_train = x_train.astype(np.float64)\n",
    "x_train, x_std, x_mean = z_score(x_train)\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(2147)\n",
    "w1 = np.random.randn(x_train.shape[1], 100)\n",
    "w2 = np.random.randn(100, 1)\n",
    "b1 = np.random.randn(100)\n",
    "b2 = np.random.randn(1)\n",
    "\n",
    "# Training loop with adjusted learning rate\n",
    "learning_rate = 0.0005 # Adjust learning rate if necessary\n",
    "for i in range(10000):\n",
    "    w1, b1, w2, b2 = gradient_descent(x_train, y_train, w1, b1, w2, b2, learning_rate)\n",
    "    current_cost = cost(x_train, y_train, w1, b1, w2, b2)\n",
    "   \n",
    "    \n",
    "\n",
    "    if np.isnan(current_cost):\n",
    "        print(\"NaN encountered in cost.\")\n",
    "        break\n",
    "\n",
    "final_cost = cost(x_train, y_train, w1, b1, w2, b2)\n",
    "print(f\"Final Cost: {final_cost}\")\n",
    "\n",
    "# Load and preprocess prediction data\n",
    "x_predict = pd.read_excel('Test data.xlsx').iloc[:, :8].to_numpy()\n",
    "x_predict = feature_changing(x_predict)\n",
    "x_predict = x_predict.astype(np.float64)\n",
    "x_predict = (x_predict - x_mean)/x_std\n",
    "ans = pd.read_excel('Test data.xlsx').iloc[:, 8].to_numpy()\n",
    "\n",
    "# Make predictions\n",
    "value1 = np.dot(x_predict, w1) + b1\n",
    "y_predict = np.dot(value1, w2) + b2\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 0\n",
    "for dim in range(len(ans)):\n",
    "    if abs(y_predict[dim] - ans[dim]) < 0.5:  # Tolerance of Â±0.5\n",
    "        print(f\"{y_predict[dim]} : {ans[dim]}\")\n",
    "        accuracy += 1\n",
    "accuracy = round(accuracy * 100 / 200.0, 2)\n",
    "ok = 'Congratulations' if accuracy > 95 else 'Optimization required'\n",
    "print(f\"{ok}, your accuracy is {accuracy}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
